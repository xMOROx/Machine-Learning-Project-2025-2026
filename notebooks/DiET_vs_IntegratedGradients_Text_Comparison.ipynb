{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù DiET vs Integrated Gradients: Token Attribution for Text Classification\n",
    "\n",
    "## A Comprehensive Comparison Study\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Machine Learning Research Team  \n",
    "**Date:** 2025-2026 Academic Year  \n",
    "**Course:** Advanced Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Abstract\n",
    "\n",
    "This notebook presents a comprehensive experimental comparison between **DiET (Distractor Erasure Tuning)** and **Integrated Gradients (IG)** for explainable AI in text classification tasks. We evaluate both methods on multiple text datasets (SST-2, IMDB, AG News) using BERT-based models and token-level attribution analysis.\n",
    "\n",
    "### üéØ Research Questions\n",
    "\n",
    "1. How do DiET token attributions compare with Integrated Gradients?\n",
    "2. Do both methods identify similar important tokens for classification?\n",
    "3. How consistent are the attributions across different text classification tasks?\n",
    "\n",
    "### üìö Reference\n",
    "\n",
    "Bhalla, U., et al. (2023). **\"Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability.\"** *NeurIPS 2023.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Check GPU Availability\n",
    "\n",
    "This notebook is optimized for **Google Colab Pro** with GPU acceleration. BERT models benefit significantly from GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and type\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üñ•Ô∏è  HARDWARE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    if gpu_memory >= 15:\n",
    "        print(\"\\nüöÄ High-memory GPU detected! Using optimal settings.\")\n",
    "        GPU_CONFIG = \"high\"\n",
    "    elif gpu_memory >= 8:\n",
    "        print(\"\\n‚ú® Standard GPU detected. Using balanced settings.\")\n",
    "        GPU_CONFIG = \"standard\"\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Low-memory GPU detected. Using memory-efficient settings.\")\n",
    "        GPU_CONFIG = \"low\"\n",
    "else:\n",
    "    print(\"‚ùå No GPU available. Training will be very slow.\")\n",
    "    print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    GPU_CONFIG = \"cpu\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nüìç Using device: {DEVICE.upper()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/xMOROx/Machine-Learning-Project-2025-2026.git\"\n",
    "REPO_DIR = \"Machine-Learning-Project-2025-2026\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(\"üì• Cloning repository...\")\n",
    "    !git clone --recursive {REPO_URL}\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"üìÅ Repository already exists. Pulling latest changes...\")\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "    !git submodule update --init --recursive\n",
    "    %cd ..\n",
    "\n",
    "%cd {REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "print(\"This may take a few minutes on first run.\\n\")\n",
    "\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets tqdm matplotlib seaborn pandas numpy scikit-learn captum\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project to path and import modules\n",
    "import sys\n",
    "sys.path.insert(0, './scripts/xai_experiments')\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(f\"üìÖ Experiment started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Experimental Configuration\n",
    "\n",
    "### 2.1 Hyperparameters and Settings\n",
    "\n",
    "We configure the experiment based on available GPU memory. Text models (BERT) require more memory than image models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration based on GPU capabilities\n",
    "if GPU_CONFIG == \"high\":  # A100, V100, etc.\n",
    "    CONFIG = {\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 3,\n",
    "        \"max_length\": 256,\n",
    "        \"max_samples\": 3000,\n",
    "        \"comparison_samples\": 100,\n",
    "        \"datasets\": [\"sst2\", \"imdb\", \"ag_news\"],\n",
    "        \"top_k\": 10,  # Number of top tokens to compare\n",
    "    }\n",
    "elif GPU_CONFIG == \"standard\":  # T4, P100\n",
    "    CONFIG = {\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 2,\n",
    "        \"max_length\": 128,\n",
    "        \"max_samples\": 2000,\n",
    "        \"comparison_samples\": 50,\n",
    "        \"datasets\": [\"sst2\", \"imdb\", \"ag_news\"],\n",
    "        \"top_k\": 5,\n",
    "    }\n",
    "elif GPU_CONFIG == \"low\":  # K80, older GPUs\n",
    "    CONFIG = {\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 2,\n",
    "        \"max_length\": 64,\n",
    "        \"max_samples\": 1000,\n",
    "        \"comparison_samples\": 30,\n",
    "        \"datasets\": [\"sst2\", \"ag_news\"],  # Skip IMDB (long texts)\n",
    "        \"top_k\": 5,\n",
    "    }\n",
    "else:  # CPU\n",
    "    CONFIG = {\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 1,\n",
    "        \"max_length\": 64,\n",
    "        \"max_samples\": 500,\n",
    "        \"comparison_samples\": 20,\n",
    "        \"datasets\": [\"sst2\"],  # Single dataset for demo\n",
    "        \"top_k\": 5,\n",
    "    }\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"‚öôÔ∏è  EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Overview\n",
    "\n",
    "We evaluate on the following text classification datasets:\n",
    "\n",
    "| Dataset | Type | Classes | Avg Length | Description |\n",
    "|---------|------|---------|------------|-------------|\n",
    "| **SST-2** | Sentiment | 2 | ~20 words | Movie review sentences |\n",
    "| **IMDB** | Sentiment | 2 | ~250 words | Full movie reviews |\n",
    "| **AG News** | Topic | 4 | ~40 words | News articles |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preview datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "def preview_dataset(dataset_name, num_samples=3):\n",
    "    \"\"\"Display sample texts from a dataset.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìö {dataset_name.upper()} Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if dataset_name == \"sst2\":\n",
    "        dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
    "        text_col = \"sentence\"\n",
    "        label_col = \"label\"\n",
    "        labels = [\"Negative\", \"Positive\"]\n",
    "    elif dataset_name == \"imdb\":\n",
    "        dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "        text_col = \"text\"\n",
    "        label_col = \"label\"\n",
    "        labels = [\"Negative\", \"Positive\"]\n",
    "    elif dataset_name == \"ag_news\":\n",
    "        dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "        text_col = \"text\"\n",
    "        label_col = \"label\"\n",
    "        labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "    \n",
    "    print(f\"Total samples: {len(dataset):,}\")\n",
    "    print(f\"Classes: {labels}\")\n",
    "    print(f\"\\nSample texts:\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        text = dataset[i][text_col]\n",
    "        label = labels[dataset[i][label_col]]\n",
    "        text_preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "        print(f\"\\n  [{label}] {text_preview}\")\n",
    "\n",
    "print(\"üìä Dataset Samples Preview\")\n",
    "for dataset in CONFIG[\"datasets\"]:\n",
    "    preview_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. DiET vs Integrated Gradients Comparison Framework\n",
    "\n",
    "### 3.1 Method Overview\n",
    "\n",
    "#### Integrated Gradients (IG)\n",
    "- **Type:** Post-hoc attribution method\n",
    "- **Approach:** Integrates gradients along path from baseline to input\n",
    "- **Advantage:** Satisfies axioms of sensitivity and implementation invariance\n",
    "- **Limitation:** May highlight features correlated but not causal\n",
    "\n",
    "#### DiET for Text\n",
    "- **Type:** Inherent interpretability via fine-tuning\n",
    "- **Approach:** Learns token-level masks that preserve predictions\n",
    "- **Advantage:** Produces discriminative token attributions\n",
    "- **Focus:** Identifies truly necessary tokens for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the comparison framework\n",
    "from experiments.xai_comparison import XAIMethodsComparison, ComparisonConfig\n",
    "\n",
    "# Create configuration\n",
    "comparison_config = ComparisonConfig(\n",
    "    device=DEVICE,\n",
    "    text_datasets=CONFIG[\"datasets\"],\n",
    "    text_batch_size=CONFIG[\"batch_size\"],\n",
    "    text_epochs=CONFIG[\"epochs\"],\n",
    "    text_max_length=CONFIG[\"max_length\"],\n",
    "    text_max_samples=CONFIG[\"max_samples\"],\n",
    "    text_comparison_samples=CONFIG[\"comparison_samples\"],\n",
    "    text_top_k=CONFIG[\"top_k\"],\n",
    "    low_vram=(GPU_CONFIG == \"low\"),\n",
    "    output_dir=\"./outputs/colab_experiments/text_comparison\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Comparison framework initialized!\")\n",
    "print(f\"üìÅ Output directory: {comparison_config.output_dir}\")\n",
    "print(f\"üî§ Model: BERT-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run Experiments\n",
    "\n",
    "‚è±Ô∏è **Estimated Time:** \n",
    "- High-memory GPU: ~45-60 minutes\n",
    "- Standard GPU: ~30-45 minutes\n",
    "- Low-memory GPU: ~20-30 minutes\n",
    "- CPU: ~2+ hours (not recommended for BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comparison\n",
    "comparison = XAIMethodsComparison(comparison_config)\n",
    "\n",
    "# Run full comparison (text only)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üöÄ STARTING DiET vs INTEGRATED GRADIENTS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìö Datasets: {CONFIG['datasets']}\")\n",
    "print(f\"üî¢ Samples per dataset: {CONFIG['max_samples']}\")\n",
    "print(f\"üìà Training epochs: {CONFIG['epochs']}\")\n",
    "print(f\"üî§ Max sequence length: {CONFIG['max_length']}\")\n",
    "print(f\"üéØ Top-k tokens to compare: {CONFIG['top_k']}\")\n",
    "print(f\"\\n‚è≥ This may take a while. Progress will be shown below...\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "results = comparison.run_full_comparison(run_images=False, run_text=True)\n",
    "end_time = datetime.now()\n",
    "\n",
    "duration = end_time - start_time\n",
    "print(f\"\\n‚úÖ Experiment completed in {duration.seconds // 60} minutes {duration.seconds % 60} seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Results Analysis\n",
    "\n",
    "### 4.1 Quantitative Results: Token Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results as DataFrame\n",
    "df = comparison.get_results_dataframe()\n",
    "\n",
    "# Filter text results only\n",
    "text_df = df[df[\"Modality\"] == \"Text\"].copy()\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä QUANTITATIVE RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(text_df) > 0:\n",
    "    print(\"\\n  Token Attribution Comparison (IG vs DiET)\")\n",
    "    print(\"  \" + \"-\" * 60)\n",
    "    \n",
    "    for _, row in text_df.iterrows():\n",
    "        overlap = row.get(\"IG-DiET Overlap\", 0)\n",
    "        samples = row.get(\"Samples Compared\", 0)\n",
    "        accuracy = row.get(\"Baseline Accuracy\", 0)\n",
    "        \n",
    "        # Interpret overlap\n",
    "        if overlap >= 0.7:\n",
    "            interpretation = \"üü¢ High agreement\"\n",
    "        elif overlap >= 0.4:\n",
    "            interpretation = \"üü° Moderate agreement\"\n",
    "        else:\n",
    "            interpretation = \"üî¥ Low agreement (methods find different features)\"\n",
    "        \n",
    "        print(f\"\\n  üìö {row['Dataset']}\")\n",
    "        print(f\"     Baseline Accuracy: {accuracy:.1f}%\")\n",
    "        print(f\"     IG-DiET Top-{CONFIG['top_k']} Overlap: {overlap:.4f}\")\n",
    "        print(f\"     Samples Compared: {samples}\")\n",
    "        print(f\"     Interpretation: {interpretation}\")\n",
    "    \n",
    "    # Summary\n",
    "    avg_overlap = text_df[\"IG-DiET Overlap\"].mean()\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"üìà Average IG-DiET Overlap: {avg_overlap:.4f}\")\n",
    "    \n",
    "    if avg_overlap >= 0.5:\n",
    "        print(\"\\n‚úÖ Methods show good agreement on important tokens\")\n",
    "    else:\n",
    "        print(\"\\nüîç Methods identify different features - DiET may find more discriminative tokens\")\n",
    "else:\n",
    "    print(\"No text results available. Please run the experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualization: Method Agreement Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "if len(text_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart: IG-DiET Overlap by dataset\n",
    "    datasets = text_df[\"Dataset\"].tolist()\n",
    "    overlaps = text_df[\"IG-DiET Overlap\"].tolist()\n",
    "    \n",
    "    colors = ['#4CAF50' if o >= 0.5 else '#FF9800' if o >= 0.3 else '#F44336' for o in overlaps]\n",
    "    \n",
    "    bars = axes[0].bar(datasets, overlaps, color=colors, alpha=0.8, edgecolor='black')\n",
    "    axes[0].axhline(y=0.5, color='gray', linestyle='--', linewidth=1, label='50% threshold')\n",
    "    axes[0].set_ylabel(f'Top-{CONFIG[\"top_k\"]} Token Overlap', fontsize=12)\n",
    "    axes[0].set_title('IG-DiET Token Attribution Agreement\\n(Higher = More Agreement)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, overlaps):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    accuracies = text_df[\"Baseline Accuracy\"].tolist()\n",
    "    \n",
    "    bars2 = axes[1].barh(datasets, accuracies, color='#2196F3', alpha=0.8, edgecolor='black')\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    axes[1].set_xlabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('BERT Baseline Classification Accuracy', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for bar, acc in zip(bars2, accuracies):\n",
    "        axes[1].text(acc + 1, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{acc:.1f}%', va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/colab_experiments/text_comparison/text_method_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìÅ Figure saved to: outputs/colab_experiments/text_comparison/text_method_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualization: Token Attribution Examples\n",
    "\n",
    "Comparing which tokens IG and DiET consider most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display token comparison HTML if available\n",
    "import glob\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "html_files = glob.glob('./outputs/colab_experiments/text_comparison/**/text_comparison.html', recursive=True)\n",
    "if html_files:\n",
    "    print(\"\\nüî§ Token Attribution Comparisons:\")\n",
    "    for path in html_files[:2]:  # Show first 2 datasets\n",
    "        dataset_name = path.split('/')[-3]\n",
    "        print(f\"\\n--- {dataset_name.upper()} ---\")\n",
    "        with open(path, 'r') as f:\n",
    "            html_content = f.read()\n",
    "        display(HTML(html_content))\n",
    "else:\n",
    "    print(\"\\nüìù Token comparison visualizations will be generated after experiment completion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualization\n",
    "try:\n",
    "    viz_files = comparison.visualize_results(save_plots=True, show=True)\n",
    "    print(\"\\nüìä Generated visualization files:\")\n",
    "    for name, path in viz_files.items():\n",
    "        print(f\"   ‚Ä¢ {name}: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Visualization generation note: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Interpretation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed interpretation analysis\n",
    "if len(text_df) > 0:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîç INTERPRETATION ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    avg_overlap = text_df[\"IG-DiET Overlap\"].mean()\n",
    "    \n",
    "    print(f\"\\nüìä Overall IG-DiET Agreement: {avg_overlap:.1%}\")\n",
    "    \n",
    "    print(\"\\nüìù What does this mean?\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if avg_overlap >= 0.7:\n",
    "        print(\"\"\"\n",
    "   ‚úÖ HIGH AGREEMENT:\n",
    "   Both IG and DiET identify similar important tokens.\n",
    "   This suggests:\n",
    "   ‚Ä¢ The model relies on genuinely discriminative features\n",
    "   ‚Ä¢ Post-hoc explanations (IG) align with inherent importance (DiET)\n",
    "   ‚Ä¢ Explanations are likely faithful to model reasoning\n",
    "        \"\"\")\n",
    "    elif avg_overlap >= 0.4:\n",
    "        print(\"\"\"\n",
    "   üü° MODERATE AGREEMENT:\n",
    "   IG and DiET partially agree on important tokens.\n",
    "   This suggests:\n",
    "   ‚Ä¢ Some tokens are universally recognized as important\n",
    "   ‚Ä¢ DiET may identify additional discriminative features\n",
    "   ‚Ä¢ IG may include correlated but not causal features\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(\"\"\"\n",
    "   üî¥ LOW AGREEMENT:\n",
    "   IG and DiET identify different tokens as important.\n",
    "   This suggests:\n",
    "   ‚Ä¢ IG may highlight spurious correlations\n",
    "   ‚Ä¢ DiET focuses on truly discriminative features\n",
    "   ‚Ä¢ Post-hoc explanations may not reflect true model reasoning\n",
    "        \"\"\")\n",
    "    \n",
    "    print(\"\\nüìå Key Insight:\")\n",
    "    print(\"   DiET is designed to find the minimal set of tokens\")\n",
    "    print(\"   that are NECESSARY for classification, while IG\")\n",
    "    print(\"   measures SENSITIVITY to each token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Discussion and Conclusions\n",
    "\n",
    "### 5.1 Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "if len(text_df) > 0:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìã EXPERIMENT SUMMARY REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    avg_overlap = text_df[\"IG-DiET Overlap\"].mean()\n",
    "    avg_accuracy = text_df[\"Baseline Accuracy\"].mean()\n",
    "    \n",
    "    print(f\"\\nüìä OVERALL STATISTICS:\")\n",
    "    print(f\"   ‚Ä¢ Datasets evaluated: {len(text_df)}\")\n",
    "    print(f\"   ‚Ä¢ Average BERT accuracy: {avg_accuracy:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Average IG-DiET overlap: {avg_overlap:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Top-k tokens compared: {CONFIG['top_k']}\")\n",
    "    \n",
    "    print(f\"\\nüîç PER-DATASET BREAKDOWN:\")\n",
    "    for _, row in text_df.iterrows():\n",
    "        overlap = row.get(\"IG-DiET Overlap\", 0)\n",
    "        status = \"üü¢\" if overlap >= 0.5 else \"üü°\" if overlap >= 0.3 else \"üî¥\"\n",
    "        print(f\"   {status} {row['Dataset']}: Overlap={overlap:.4f}, Accuracy={row['Baseline Accuracy']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìù KEY CONCLUSIONS:\")\n",
    "    print(f\"   1. BERT achieves strong baseline performance ({avg_accuracy:.1f}% avg)\")\n",
    "    print(f\"   2. IG-DiET agreement varies by dataset complexity\")\n",
    "    if avg_overlap >= 0.5:\n",
    "        print(f\"   3. Both methods generally agree on important tokens\")\n",
    "    else:\n",
    "        print(f\"   3. Methods identify different features - suggests IG may include spurious tokens\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Theoretical Implications\n",
    "\n",
    "**Why might IG and DiET disagree?**\n",
    "\n",
    "1. **Sensitivity vs. Necessity:**\n",
    "   - IG measures how sensitive predictions are to each token\n",
    "   - DiET identifies which tokens are *necessary* for the prediction\n",
    "\n",
    "2. **Spurious Correlations:**\n",
    "   - IG may highlight tokens correlated with labels but not causal\n",
    "   - DiET's distractor erasure removes such spurious features\n",
    "\n",
    "3. **Baseline Dependency:**\n",
    "   - IG attributions depend on the choice of baseline (zero embedding)\n",
    "   - DiET learns a data-driven importance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Limitations and Future Work\n",
    "\n",
    "**Limitations:**\n",
    "- Limited fine-tuning epochs due to computational constraints\n",
    "- Top-k comparison may miss nuanced differences\n",
    "- Results may vary with different random seeds\n",
    "\n",
    "**Future Work:**\n",
    "- Extend to other transformer architectures (RoBERTa, DistilBERT)\n",
    "- Compare with other attribution methods (LIME, SHAP, Attention)\n",
    "- Conduct human evaluation of explanations\n",
    "- Investigate relationship between overlap and model faithfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Save Results and Export Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "import json\n",
    "\n",
    "# Create comprehensive results dictionary\n",
    "full_results = {\n",
    "    \"experiment\": \"DiET vs Integrated Gradients Text Comparison\",\n",
    "    \"date\": datetime.now().isoformat(),\n",
    "    \"configuration\": CONFIG,\n",
    "    \"device\": DEVICE,\n",
    "    \"gpu_config\": GPU_CONFIG,\n",
    "    \"results\": results,\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_path = \"./outputs/colab_experiments/text_comparison/full_results.json\"\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_serializable(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(make_serializable(full_results), f, indent=2)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "if len(text_df) > 0:\n",
    "    text_df.to_csv('./outputs/colab_experiments/text_comparison/results_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Results saved successfully!\")\n",
    "print(f\"   üìÑ JSON: {results_path}\")\n",
    "print(f\"   üìä CSV: ./outputs/colab_experiments/text_comparison/results_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate printable report\n",
    "report_content = f\"\"\"\n",
    "================================================================================\n",
    "                    DiET vs INTEGRATED GRADIENTS COMPARISON\n",
    "                         Text Classification Analysis\n",
    "================================================================================\n",
    "\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Device: {DEVICE} ({GPU_CONFIG})\n",
    "\n",
    "CONFIGURATION:\n",
    "  ‚Ä¢ Datasets: {', '.join(CONFIG['datasets'])}\n",
    "  ‚Ä¢ Model: BERT-base-uncased\n",
    "  ‚Ä¢ Max sequence length: {CONFIG['max_length']}\n",
    "  ‚Ä¢ Training epochs: {CONFIG['epochs']}\n",
    "  ‚Ä¢ Batch size: {CONFIG['batch_size']}\n",
    "  ‚Ä¢ Comparison samples: {CONFIG['comparison_samples']}\n",
    "  ‚Ä¢ Top-k tokens: {CONFIG['top_k']}\n",
    "\n",
    "RESULTS:\n",
    "\"\"\"\n",
    "\n",
    "if len(text_df) > 0:\n",
    "    for _, row in text_df.iterrows():\n",
    "        report_content += f\"\"\"\n",
    "  {row['Dataset'].upper()}:\n",
    "    ‚Ä¢ Baseline Accuracy: {row['Baseline Accuracy']:.1f}%\n",
    "    ‚Ä¢ IG-DiET Overlap: {row.get('IG-DiET Overlap', 0):.4f}\n",
    "\"\"\"\n",
    "    \n",
    "    avg_overlap = text_df[\"IG-DiET Overlap\"].mean()\n",
    "    avg_accuracy = text_df[\"Baseline Accuracy\"].mean()\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "SUMMARY:\n",
    "  ‚Ä¢ Average Accuracy: {avg_accuracy:.1f}%\n",
    "  ‚Ä¢ Average IG-DiET Overlap: {avg_overlap:.4f}\n",
    "  \n",
    "INTERPRETATION:\n",
    "  {'High agreement - Both methods identify similar important tokens' if avg_overlap >= 0.5 else 'Lower agreement - Methods may identify different features'}\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "with open('./outputs/colab_experiments/text_comparison/experiment_report.txt', 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(report_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Create zip of all results\n",
    "    !zip -r text_comparison_results.zip ./outputs/colab_experiments/text_comparison/\n",
    "    \n",
    "    print(\"\\nüì• Download your results:\")\n",
    "    files.download('text_comparison_results.zip')\n",
    "except:\n",
    "    print(\"\\nüìÅ Results are saved locally in: ./outputs/colab_experiments/text_comparison/\")\n",
    "    print(\"   (Download option only available in Google Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "1. Bhalla, U., et al. (2023). \"Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability.\" *NeurIPS 2023.*\n",
    "\n",
    "2. Sundararajan, M., Taly, A., & Yan, Q. (2017). \"Axiomatic Attribution for Deep Networks.\" *ICML 2017.*\n",
    "\n",
    "3. Devlin, J., et al. (2019). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" *NAACL 2019.*\n",
    "\n",
    "4. Socher, R., et al. (2013). \"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.\" *EMNLP 2013.* (SST-2)\n",
    "\n",
    "5. Maas, A., et al. (2011). \"Learning Word Vectors for Sentiment Analysis.\" *ACL 2011.* (IMDB)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 1.0  \n",
    "**Last Updated:** 2025-2026 Academic Year  \n",
    "**Repository:** [github.com/xMOROx/Machine-Learning-Project-2025-2026](https://github.com/xMOROx/Machine-Learning-Project-2025-2026)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
