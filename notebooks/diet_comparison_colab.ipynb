{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header-cell"
   },
   "source": [
    "# üî¨ DiET vs Basic XAI Methods - Comprehensive Comparison Framework\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xMOROx/Machine-Learning-Project-2025-2026/blob/main/notebooks/diet_comparison_colab.ipynb)\n",
    "\n",
    "**A Hands-on Tutorial for Reproducing XAI Comparison Results**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What You'll Learn\n",
    "\n",
    "This notebook provides a comprehensive, reproducible comparison of:\n",
    "\n",
    "| Modality | Methods Compared | Datasets |\n",
    "|----------|------------------|----------|\n",
    "| **Images** | DiET vs GradCAM | CIFAR-10, CIFAR-100, SVHN, Fashion-MNIST |\n",
    "| **Text** | DiET vs Integrated Gradients | SST-2, IMDB, AG News |\n",
    "\n",
    "### üìñ Topics Covered\n",
    "\n",
    "1. **Environment Setup** - Install dependencies and clone repository\n",
    "2. **Quick Start** - Run comparison with minimal code\n",
    "3. **Dataset Exploration** - Explore all supported datasets\n",
    "4. **Metrics Deep Dive** - Understand evaluation metrics\n",
    "5. **Full Comparison** - Run multi-dataset experiments\n",
    "6. **Visualization** - Create publication-ready plots\n",
    "7. **Custom Experiments** - Advanced configuration\n",
    "8. **Results Analysis** - Interpret and export results\n",
    "\n",
    "### üìö References\n",
    "\n",
    "- **DiET Paper**: Bhalla et al., \"Discriminative Feature Attributions\", NeurIPS 2023\n",
    "- **GradCAM**: Selvaraju et al., \"Grad-CAM: Visual Explanations from Deep Networks\", ICCV 2017\n",
    "- **Integrated Gradients**: Sundararajan et al., \"Axiomatic Attribution for Deep Networks\", ICML 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üöÄ Part 1: Environment Setup\n",
    "\n",
    "First, let's set up the environment for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower.\")\n",
    "    print(\"   Go to Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/xMOROx/Machine-Learning-Project-2025-2026.git\"\n",
    "REPO_DIR = \"Machine-Learning-Project-2025-2026\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL}\n",
    "    print(f\"‚úÖ Repository cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repository already exists at {REPO_DIR}\")\n",
    "    # Pull latest changes\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Core dependencies\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers datasets\n",
    "!pip install -q captum  # For Integrated Gradients\n",
    "!pip install -q matplotlib seaborn pandas numpy\n",
    "!pip install -q tqdm scikit-learn\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-path"
   },
   "outputs": [],
   "source": [
    "# Add project to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from scripts.xai_experiments import XAIMethodsComparison, ComparisonConfig\n",
    "    from scripts.xai_experiments.datasets import SUPPORTED_IMAGE_DATASETS, SUPPORTED_TEXT_DATASETS\n",
    "    from scripts.xai_experiments.metrics import AttributionMetrics, PixelPerturbation, AOPC\n",
    "    from scripts.xai_experiments.visualization import ComparisonVisualizer\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   Make sure you're in the repository directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-output-dirs"
   },
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"./outputs/colab_experiments\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/visualizations\", exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quickstart-section"
   },
   "source": [
    "---\n",
    "\n",
    "# ‚ö° Part 2: Quick Start\n",
    "\n",
    "Run a complete comparison with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quickstart-demo"
   },
   "outputs": [],
   "source": [
    "# Quick start - minimal configuration for a fast demo\n",
    "from scripts.xai_experiments import XAIMethodsComparison, ComparisonConfig\n",
    "\n",
    "# Use reduced settings for quick demo (full comparison later)\n",
    "quick_config = ComparisonConfig(\n",
    "    device=DEVICE,\n",
    "    \n",
    "    # Use only one dataset each for quick demo\n",
    "    image_datasets=[\"cifar10\"],\n",
    "    text_datasets=[\"sst2\"],\n",
    "    \n",
    "    # Reduced training for speed\n",
    "    image_epochs=2,\n",
    "    image_max_samples=1000,\n",
    "    image_comparison_samples=20,\n",
    "    \n",
    "    text_epochs=1,\n",
    "    text_max_samples=500,\n",
    "    text_comparison_samples=10,\n",
    "    text_top_k=5,  # Show top 5 important tokens\n",
    "    \n",
    "    output_dir=f\"{OUTPUT_DIR}/quick_demo\"\n",
    ")\n",
    "\n",
    "print(\"üìã Quick Demo Configuration:\")\n",
    "print(f\"   Device: {quick_config.device}\")\n",
    "print(f\"   Image datasets: {quick_config.image_datasets}\")\n",
    "print(f\"   Text datasets: {quick_config.text_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-quick-demo"
   },
   "outputs": [],
   "source": [
    "# Run quick demo (images only for speed)\n",
    "quick_comparison = XAIMethodsComparison(quick_config)\n",
    "\n",
    "print(\"üöÄ Running quick image comparison (DiET vs GradCAM)...\")\n",
    "quick_results = quick_comparison.run_full_comparison(\n",
    "    run_images=True,\n",
    "    run_text=False,  # Skip text for quick demo\n",
    "    skip_training=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Quick demo complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view-quick-results"
   },
   "outputs": [],
   "source": [
    "# View results as DataFrame\n",
    "df = quick_comparison.get_results_dataframe()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "datasets-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üìä Part 3: Dataset Exploration\n",
    "\n",
    "Let's explore all the datasets supported by the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list-datasets"
   },
   "outputs": [],
   "source": [
    "# List all supported datasets\n",
    "from scripts.xai_experiments.datasets import SUPPORTED_IMAGE_DATASETS, SUPPORTED_TEXT_DATASETS\n",
    "\n",
    "print(\"üñºÔ∏è Supported Image Datasets:\")\n",
    "print(\"-\" * 50)\n",
    "for name, info in SUPPORTED_IMAGE_DATASETS.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {info['description']} ({info['num_classes']} classes)\")\n",
    "\n",
    "print(\"\\nüìù Supported Text Datasets:\")\n",
    "print(\"-\" * 50)\n",
    "for name, info in SUPPORTED_TEXT_DATASETS.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {info['description']} ({info['num_classes']} classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-image-datasets"
   },
   "outputs": [],
   "source": [
    "# Load and visualize image datasets\n",
    "from scripts.xai_experiments.datasets import get_image_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "image_datasets = [\"cifar10\", \"cifar100\", \"svhn\", \"fashion_mnist\"]\n",
    "\n",
    "for idx, dataset_name in enumerate(image_datasets):\n",
    "    try:\n",
    "        # Load dataset\n",
    "        train_loader, test_loader, num_classes = get_image_dataset(\n",
    "            dataset_name,\n",
    "            batch_size=16,\n",
    "            max_samples=100\n",
    "        )\n",
    "        \n",
    "        # Get sample batch\n",
    "        images, labels = next(iter(train_loader))\n",
    "        \n",
    "        # Plot first image\n",
    "        ax = axes[0, idx]\n",
    "        img = images[0].permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize for display\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{dataset_name.upper()}\\n(Class: {labels[0].item()})\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Plot grid of images\n",
    "        ax = axes[1, idx]\n",
    "        grid_size = 4\n",
    "        grid_img = images[:grid_size*grid_size].reshape(grid_size, grid_size, *images.shape[1:])\n",
    "        ax.text(0.5, 0.5, f\"{num_classes} classes\", ha='center', va='center', fontsize=14)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        print(f\"‚úÖ {dataset_name}: {len(train_loader.dataset)} train samples, {num_classes} classes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {dataset_name}: {e}\")\n",
    "        axes[0, idx].text(0.5, 0.5, f\"Error loading\\n{dataset_name}\", ha='center', va='center')\n",
    "        axes[0, idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Supported Image Datasets\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/visualizations/dataset_samples.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-text-datasets"
   },
   "outputs": [],
   "source": [
    "# Load and explore text datasets\n",
    "from scripts.xai_experiments.datasets import get_text_dataset\n",
    "\n",
    "text_datasets = [\"sst2\", \"imdb\", \"ag_news\"]\n",
    "\n",
    "print(\"üìù Text Dataset Samples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in text_datasets:\n",
    "    try:\n",
    "        train_data, test_data, num_classes = get_text_dataset(\n",
    "            dataset_name,\n",
    "            max_samples=10\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüî§ {dataset_name.upper()} ({num_classes} classes)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Show sample texts\n",
    "        for i, sample in enumerate(train_data[:3]):\n",
    "            text = sample['text'][:100] + \"...\" if len(sample['text']) > 100 else sample['text']\n",
    "            label = sample['label']\n",
    "            print(f\"  [{label}] {text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metrics-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üìè Part 4: Metrics Deep Dive\n",
    "\n",
    "Understand the evaluation metrics used to compare XAI methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explain-metrics"
   },
   "outputs": [],
   "source": [
    "# Display metrics explanation\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "metrics_explanation = \"\"\"\n",
    "## üìä Evaluation Metrics\n",
    "\n",
    "### Image Attribution Metrics\n",
    "\n",
    "| Metric | Description | Higher = Better? |\n",
    "|--------|-------------|------------------|\n",
    "| **Pixel Perturbation (Keep)** | Keep top-k% most important pixels, measure accuracy | ‚úÖ Yes |\n",
    "| **Pixel Perturbation (Remove)** | Remove top-k% most important pixels, measure accuracy drop | ‚ùå No (lower = better) |\n",
    "| **AOPC** | Area Over Perturbation Curve - aggregate measure | ‚úÖ Yes |\n",
    "| **Insertion** | Progressively add pixels in importance order | ‚úÖ Yes |\n",
    "| **Deletion** | Progressively remove pixels in importance order | ‚ùå No |\n",
    "| **Faithfulness** | Correlation between attribution and model sensitivity | ‚úÖ Yes |\n",
    "\n",
    "### Text Attribution Metrics\n",
    "\n",
    "| Metric | Description | Range |\n",
    "|--------|-------------|-------|\n",
    "| **Top-k Token Overlap** | Agreement between IG and DiET on most important tokens | 0-1 |\n",
    "| **Token Attribution Score** | Importance score for each token | Any |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **DiET Better**: When DiET achieves higher scores on perturbation-based metrics\n",
    "- **High Overlap**: IG and DiET agree on important features (good for validation)\n",
    "- **Low Overlap**: DiET identifies different discriminative features than IG\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(metrics_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo-metrics"
   },
   "outputs": [],
   "source": [
    "# Demonstrate metrics computation\n",
    "from scripts.xai_experiments.metrics import PixelPerturbation, AOPC, InsertionDeletion\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"üî¨ Metrics Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create dummy data for demonstration\n",
    "batch_size = 4\n",
    "dummy_images = torch.randn(batch_size, 3, 32, 32)\n",
    "dummy_attributions = torch.rand(batch_size, 32, 32)  # Random attributions\n",
    "\n",
    "print(f\"\\nüìä Sample shapes:\")\n",
    "print(f\"   Images: {dummy_images.shape}\")\n",
    "print(f\"   Attributions: {dummy_attributions.shape}\")\n",
    "\n",
    "# Show how metrics work conceptually\n",
    "print(\"\\nüìê Metric Computation:\")\n",
    "print(\"\\n   1. Pixel Perturbation:\")\n",
    "print(\"      - Rank pixels by attribution importance\")\n",
    "print(\"      - Keep/remove top k% of pixels\")\n",
    "print(\"      - Measure model accuracy change\")\n",
    "\n",
    "print(\"\\n   2. AOPC (Area Over Perturbation Curve):\")\n",
    "print(\"      - Compute perturbation at multiple thresholds\")\n",
    "print(\"      - Calculate area under the curve\")\n",
    "print(\"      - Higher AOPC = more faithful attributions\")\n",
    "\n",
    "print(\"\\n   3. Insertion/Deletion:\")\n",
    "print(\"      - Start with blank/full image\")\n",
    "print(\"      - Progressively add/remove pixels\")\n",
    "print(\"      - Track prediction confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "full-comparison-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üî¨ Part 5: Full Multi-Dataset Comparison\n",
    "\n",
    "Run the complete comparison across all datasets for robust results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full-config"
   },
   "outputs": [],
   "source": [
    "# Full comparison configuration\n",
    "from scripts.xai_experiments import XAIMethodsComparison, ComparisonConfig\n",
    "\n",
    "# Configure for comprehensive comparison\n",
    "# Adjust these based on your GPU memory and time constraints\n",
    "\n",
    "full_config = ComparisonConfig(\n",
    "    device=DEVICE,\n",
    "    \n",
    "    # === Image Datasets ===\n",
    "    # Use all 4 datasets for robust comparison\n",
    "    image_datasets=[\"cifar10\", \"cifar100\", \"svhn\", \"fashion_mnist\"],\n",
    "    image_model_type=\"resnet\",\n",
    "    image_batch_size=64 if DEVICE == \"cuda\" else 16,\n",
    "    image_epochs=5,\n",
    "    image_max_samples=5000,  # Training samples per dataset\n",
    "    image_comparison_samples=100,  # Samples for XAI comparison\n",
    "    \n",
    "    # === Text Datasets ===\n",
    "    # Use all 3 datasets\n",
    "    text_datasets=[\"sst2\", \"imdb\", \"ag_news\"],\n",
    "    text_model_name=\"bert-base-uncased\",\n",
    "    text_max_length=128,\n",
    "    text_epochs=3,\n",
    "    text_max_samples=2000,\n",
    "    text_comparison_samples=50,\n",
    "    text_top_k=10,  # Show top 10 tokens\n",
    "    \n",
    "    # === DiET Settings ===\n",
    "    diet_upsample_factor=4,\n",
    "    diet_rounding_steps=2,\n",
    "    \n",
    "    # === Metric Settings ===\n",
    "    perturbation_percentages=[5, 10, 20, 30, 50, 70, 90],\n",
    "    insertion_deletion_steps=50,\n",
    "    aopc_steps=10,\n",
    "    faithfulness_samples=30,\n",
    "    \n",
    "    # === Output ===\n",
    "    output_dir=f\"{OUTPUT_DIR}/full_comparison\",\n",
    "    save_visualizations=True\n",
    ")\n",
    "\n",
    "print(\"üìã Full Comparison Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Device: {full_config.device}\")\n",
    "print(f\"\\nüñºÔ∏è Image Datasets: {', '.join(full_config.image_datasets)}\")\n",
    "print(f\"   Epochs: {full_config.image_epochs}\")\n",
    "print(f\"   Samples/dataset: {full_config.image_max_samples}\")\n",
    "print(f\"   Comparison samples: {full_config.image_comparison_samples}\")\n",
    "print(f\"\\nüìù Text Datasets: {', '.join(full_config.text_datasets)}\")\n",
    "print(f\"   Epochs: {full_config.text_epochs}\")\n",
    "print(f\"   Top-k tokens: {full_config.text_top_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-full-comparison"
   },
   "outputs": [],
   "source": [
    "# Run the full comparison\n",
    "# ‚ö†Ô∏è This may take 30-60 minutes depending on GPU\n",
    "\n",
    "full_comparison = XAIMethodsComparison(full_config)\n",
    "\n",
    "print(\"üöÄ Starting full comparison...\")\n",
    "print(\"   This may take 30-60 minutes.\")\n",
    "print(\"   Training checkpoints are saved automatically.\")\n",
    "print(\"   If interrupted, re-run this cell to resume.\\n\")\n",
    "\n",
    "# Run both image and text experiments\n",
    "full_results = full_comparison.run_full_comparison(\n",
    "    run_images=True,\n",
    "    run_text=True,\n",
    "    skip_training=False  # Set to True to use cached models\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ FULL COMPARISON COMPLETE!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results-dataframe"
   },
   "outputs": [],
   "source": [
    "# View comprehensive results as DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = full_comparison.get_results_dataframe()\n",
    "\n",
    "print(\"üìä Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "display(df)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = f\"{OUTPUT_DIR}/full_comparison/results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nüìÅ Results saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üìà Part 6: Visualization\n",
    "\n",
    "Create publication-ready visualizations of the comparison results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-visualizations"
   },
   "outputs": [],
   "source": [
    "# Generate all visualizations\n",
    "viz_files = full_comparison.visualize_results(save_plots=True, show=True)\n",
    "\n",
    "print(\"\\nüìä Generated Visualization Files:\")\n",
    "for name, path in viz_files.items():\n",
    "    print(f\"   ‚Ä¢ {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom-bar-chart"
   },
   "outputs": [],
   "source": [
    "# Create custom comparison bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract results for plotting\n",
    "image_results = full_results.get(\"image_experiments\", {})\n",
    "\n",
    "if image_results:\n",
    "    datasets = []\n",
    "    gradcam_scores = []\n",
    "    diet_scores = []\n",
    "    \n",
    "    for ds_name, ds_data in image_results.items():\n",
    "        if \"error\" not in ds_data:\n",
    "            datasets.append(ds_name.upper())\n",
    "            gradcam_scores.append(ds_data.get(\"gradcam_mean_score\", 0))\n",
    "            diet_scores.append(ds_data.get(\"diet_mean_score\", 0))\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, gradcam_scores, width, label='GradCAM', color='#2196F3', edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, diet_scores, width, label='DiET', color='#4CAF50', edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('Dataset', fontsize=12)\n",
    "    ax.set_ylabel('Pixel Perturbation Score', fontsize=12)\n",
    "    ax.set_title('DiET vs GradCAM: Image Attribution Quality\\n(Higher = Better)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/visualizations/image_comparison_bar.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No image results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "text-comparison-chart"
   },
   "outputs": [],
   "source": [
    "# Create text comparison visualization\n",
    "text_results = full_results.get(\"text_experiments\", {})\n",
    "\n",
    "if text_results:\n",
    "    datasets = []\n",
    "    overlaps = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for ds_name, ds_data in text_results.items():\n",
    "        if \"error\" not in ds_data:\n",
    "            datasets.append(ds_name.upper())\n",
    "            overlaps.append(ds_data.get(\"ig_diet_overlap\", 0))\n",
    "            accuracies.append(ds_data.get(\"baseline_accuracy\", 0))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Token overlap\n",
    "    colors = ['#FF9800' if o > 0.5 else '#F44336' for o in overlaps]\n",
    "    bars = axes[0].barh(datasets, overlaps, color=colors, edgecolor='black')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_xlabel('Top-k Token Overlap', fontsize=12)\n",
    "    axes[0].set_title('IG vs DiET Token Agreement\\n(Higher = More Agreement)', fontsize=12, fontweight='bold')\n",
    "    axes[0].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "    \n",
    "    for bar, val in zip(bars, overlaps):\n",
    "        axes[0].text(val + 0.02, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{val:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    # Baseline accuracy\n",
    "    bars = axes[1].barh(datasets, accuracies, color='#9C27B0', edgecolor='black')\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    axes[1].set_xlabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('Model Baseline Accuracy', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for bar, val in zip(bars, accuracies):\n",
    "        axes[1].text(val + 1, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{val:.1f}%', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/visualizations/text_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No text results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary-dashboard"
   },
   "outputs": [],
   "source": [
    "# Create summary dashboard\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Title\n",
    "fig.suptitle('DiET vs Basic XAI Methods - Comparison Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# 1. Image comparison (top left)\n",
    "ax1 = fig.add_subplot(gs[0, 0:2])\n",
    "if image_results:\n",
    "    x = np.arange(len(datasets))\n",
    "    ax1.bar(x - 0.2, gradcam_scores, 0.4, label='GradCAM', color='#2196F3')\n",
    "    ax1.bar(x + 0.2, diet_scores, 0.4, label='DiET', color='#4CAF50')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([ds for ds in image_results.keys()])\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Image Attribution Quality')\n",
    "    ax1.legend()\n",
    "\n",
    "# 2. Win/Loss summary (top right)\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "if image_results:\n",
    "    diet_wins = sum(1 for ds in image_results.values() if ds.get('diet_better', False))\n",
    "    total = len([ds for ds in image_results.values() if 'error' not in ds])\n",
    "    ax2.pie([diet_wins, total - diet_wins], \n",
    "            labels=['DiET Better', 'GradCAM Better'],\n",
    "            colors=['#4CAF50', '#2196F3'],\n",
    "            autopct='%1.0f%%',\n",
    "            startangle=90)\n",
    "    ax2.set_title('Image: Win Rate')\n",
    "\n",
    "# 3. Text overlap (bottom left)\n",
    "ax3 = fig.add_subplot(gs[1, 0:2])\n",
    "if text_results:\n",
    "    ds_names = list(text_results.keys())\n",
    "    overlaps = [text_results[ds].get('ig_diet_overlap', 0) for ds in ds_names if 'error' not in text_results[ds]]\n",
    "    ax3.barh(ds_names, overlaps, color='#FF9800')\n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.set_xlabel('Token Overlap')\n",
    "    ax3.set_title('Text: IG-DiET Agreement')\n",
    "\n",
    "# 4. Summary stats (bottom right)\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "ax4.axis('off')\n",
    "summary_text = \"üìä Summary Statistics\\n\\n\"\n",
    "if image_results:\n",
    "    avg_improvement = np.mean([ds.get('improvement', 0) for ds in image_results.values() if 'error' not in ds])\n",
    "    summary_text += f\"Image Avg Improvement: {avg_improvement:.4f}\\n\"\n",
    "if text_results:\n",
    "    avg_overlap = np.mean([ds.get('ig_diet_overlap', 0) for ds in text_results.values() if 'error' not in ds])\n",
    "    summary_text += f\"Text Avg Overlap: {avg_overlap:.4f}\\n\"\n",
    "summary_text += f\"\\nTotal Datasets: {len(image_results) + len(text_results)}\"\n",
    "ax4.text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center', \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig(f\"{OUTPUT_DIR}/visualizations/comparison_dashboard.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom-experiments-section"
   },
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è Part 7: Custom Experiments\n",
    "\n",
    "Learn how to customize experiments for specific research needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single-dataset-experiment"
   },
   "outputs": [],
   "source": [
    "# Run comparison on a single dataset\n",
    "from scripts.xai_experiments import XAIMethodsComparison, ComparisonConfig\n",
    "\n",
    "# Focus on CIFAR-100 only\n",
    "cifar100_config = ComparisonConfig(\n",
    "    device=DEVICE,\n",
    "    image_datasets=[\"cifar100\"],\n",
    "    image_epochs=3,\n",
    "    image_max_samples=3000,\n",
    "    image_comparison_samples=50,\n",
    "    output_dir=f\"{OUTPUT_DIR}/cifar100_only\"\n",
    ")\n",
    "\n",
    "print(\"Running CIFAR-100 focused experiment...\")\n",
    "cifar100_comparison = XAIMethodsComparison(cifar100_config)\n",
    "cifar100_results = cifar100_comparison.run_full_comparison(run_images=True, run_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "text-only-experiment"
   },
   "outputs": [],
   "source": [
    "# Text-only experiment with higher top-k\n",
    "text_config = ComparisonConfig(\n",
    "    device=DEVICE,\n",
    "    text_datasets=[\"sst2\", \"imdb\"],\n",
    "    text_epochs=2,\n",
    "    text_max_samples=1000,\n",
    "    text_comparison_samples=30,\n",
    "    text_top_k=15,  # Show top 15 tokens\n",
    "    output_dir=f\"{OUTPUT_DIR}/text_only\"\n",
    ")\n",
    "\n",
    "print(\"Running text-only experiment with top-15 tokens...\")\n",
    "text_comparison = XAIMethodsComparison(text_config)\n",
    "text_results = text_comparison.run_full_comparison(run_images=False, run_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "resume-training"
   },
   "outputs": [],
   "source": [
    "# Demonstrate resumable training (checkpoint support)\n",
    "from scripts.xai_experiments import CheckpointManager\n",
    "\n",
    "# List available checkpoints\n",
    "ckpt_dir = f\"{OUTPUT_DIR}/full_comparison/checkpoints\"\n",
    "ckpt_manager = CheckpointManager(ckpt_dir)\n",
    "\n",
    "print(\"üìÅ Available Checkpoints:\")\n",
    "checkpoints = ckpt_manager.list_checkpoints()\n",
    "if checkpoints:\n",
    "    for ckpt in checkpoints:\n",
    "        print(f\"   ‚Ä¢ {ckpt}\")\n",
    "else:\n",
    "    print(\"   No checkpoints found.\")\n",
    "\n",
    "print(\"\\nüí° Tip: If training is interrupted, it will resume from the last checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üìù Part 8: Results Analysis & Export\n",
    "\n",
    "Analyze results and export for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "statistical-analysis"
   },
   "outputs": [],
   "source": [
    "# Statistical analysis of results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = full_comparison.get_results_dataframe()\n",
    "\n",
    "print(\"üìä Statistical Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Image results statistics\n",
    "image_df = df[df['Modality'] == 'Image']\n",
    "if not image_df.empty:\n",
    "    print(\"\\nüñºÔ∏è Image Results:\")\n",
    "    print(f\"   Datasets tested: {len(image_df)}\")\n",
    "    print(f\"   DiET wins: {image_df['DiET Better'].sum()} / {len(image_df)}\")\n",
    "    print(f\"   Mean GradCAM score: {image_df['GradCAM Score'].mean():.4f} ¬± {image_df['GradCAM Score'].std():.4f}\")\n",
    "    print(f\"   Mean DiET score: {image_df['DiET Score'].mean():.4f} ¬± {image_df['DiET Score'].std():.4f}\")\n",
    "    print(f\"   Mean improvement: {image_df['Improvement'].mean():.4f}\")\n",
    "\n",
    "# Text results statistics\n",
    "text_df = df[df['Modality'] == 'Text']\n",
    "if not text_df.empty:\n",
    "    print(\"\\nüìù Text Results:\")\n",
    "    print(f\"   Datasets tested: {len(text_df)}\")\n",
    "    print(f\"   Mean IG-DiET overlap: {text_df['IG-DiET Overlap'].mean():.4f} ¬± {text_df['IG-DiET Overlap'].std():.4f}\")\n",
    "    print(f\"   Mean accuracy: {text_df['Baseline Accuracy'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-results"
   },
   "outputs": [],
   "source": [
    "# Export results in multiple formats\n",
    "import json\n",
    "\n",
    "export_dir = f\"{OUTPUT_DIR}/exports\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# 1. CSV export\n",
    "csv_path = f\"{export_dir}/results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ CSV saved: {csv_path}\")\n",
    "\n",
    "# 2. JSON export (full results)\n",
    "json_path = f\"{export_dir}/results.json\"\n",
    "# Convert numpy types for JSON serialization\n",
    "def convert_for_json(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(convert_for_json(full_results), f, indent=2)\n",
    "print(f\"‚úÖ JSON saved: {json_path}\")\n",
    "\n",
    "# 3. LaTeX table\n",
    "latex_path = f\"{export_dir}/results_table.tex\"\n",
    "latex_table = df.to_latex(index=False, float_format=\"%.4f\")\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"‚úÖ LaTeX table saved: {latex_path}\")\n",
    "\n",
    "# 4. Markdown summary\n",
    "md_path = f\"{export_dir}/results_summary.md\"\n",
    "md_content = f\"\"\"# DiET vs Basic XAI Methods - Results Summary\n",
    "\n",
    "## Overview\n",
    "\n",
    "| Modality | Datasets | DiET Wins | Avg Improvement |\n",
    "|----------|----------|-----------|------------------|\n",
    "| Image    | {len(image_df)} | {image_df['DiET Better'].sum() if not image_df.empty else 0} | {image_df['Improvement'].mean():.4f if not image_df.empty else 'N/A'} |\n",
    "| Text     | {len(text_df)} | N/A | {text_df['IG-DiET Overlap'].mean():.4f if not text_df.empty else 'N/A'} (overlap) |\n",
    "\n",
    "## Detailed Results\n",
    "\n",
    "{df.to_markdown(index=False)}\n",
    "\n",
    "Generated: {full_results.get('timestamp', 'N/A')}\n",
    "\"\"\"\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write(md_content)\n",
    "print(f\"‚úÖ Markdown summary saved: {md_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-results"
   },
   "outputs": [],
   "source": [
    "# Download results (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Zip all exports\n",
    "    !cd {OUTPUT_DIR} && zip -r exports.zip exports visualizations\n",
    "    \n",
    "    # Download\n",
    "    files.download(f\"{OUTPUT_DIR}/exports.zip\")\n",
    "    print(\"üì• Download started!\")\n",
    "except ImportError:\n",
    "    print(\"üí° Not running in Colab. Results are saved locally at:\")\n",
    "    print(f\"   {OUTPUT_DIR}/exports/\")\n",
    "    print(f\"   {OUTPUT_DIR}/visualizations/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üéØ Part 9: Hands-On Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise-1"
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Vary the number of comparison samples\n",
    "# Question: How does the number of samples affect result stability?\n",
    "\n",
    "# TODO: Uncomment and modify\n",
    "# sample_sizes = [10, 25, 50, 100]\n",
    "# results_by_samples = {}\n",
    "# \n",
    "# for n_samples in sample_sizes:\n",
    "#     config = ComparisonConfig(\n",
    "#         device=DEVICE,\n",
    "#         image_datasets=[\"cifar10\"],\n",
    "#         image_comparison_samples=n_samples,\n",
    "#         image_epochs=2,\n",
    "#         output_dir=f\"{OUTPUT_DIR}/exercise1/samples_{n_samples}\"\n",
    "#     )\n",
    "#     comparison = XAIMethodsComparison(config)\n",
    "#     results = comparison.run_full_comparison(run_text=False)\n",
    "#     results_by_samples[n_samples] = results\n",
    "\n",
    "print(\"üí° Exercise 1: Vary sample sizes to study result stability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise-2"
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Compare different model architectures\n",
    "# Question: Does the model architecture affect XAI method performance?\n",
    "\n",
    "# TODO: Uncomment and modify\n",
    "# model_types = [\"resnet\", \"vgg\", \"densenet\"]  # If supported\n",
    "# results_by_model = {}\n",
    "# \n",
    "# for model_type in model_types:\n",
    "#     config = ComparisonConfig(\n",
    "#         device=DEVICE,\n",
    "#         image_model_type=model_type,\n",
    "#         ...\n",
    "#     )\n",
    "#     ...\n",
    "\n",
    "print(\"üí° Exercise 2: Compare XAI methods across different model architectures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise-3"
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Analyze text top-k sensitivity\n",
    "# Question: How does changing top-k affect IG-DiET overlap?\n",
    "\n",
    "# TODO: Uncomment and modify\n",
    "# top_k_values = [3, 5, 10, 15, 20]\n",
    "# overlaps_by_k = []\n",
    "# \n",
    "# for k in top_k_values:\n",
    "#     config = ComparisonConfig(\n",
    "#         device=DEVICE,\n",
    "#         text_datasets=[\"sst2\"],\n",
    "#         text_top_k=k,\n",
    "#         ...\n",
    "#     )\n",
    "#     ...\n",
    "# \n",
    "# Plot: top-k vs overlap\n",
    "\n",
    "print(\"üí° Exercise 3: Study how top-k affects IG-DiET token agreement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion-section"
   },
   "source": [
    "---\n",
    "\n",
    "# üéì Conclusion\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "1. **DiET** (Discriminative Feature Attribution) provides an alternative approach to traditional XAI methods\n",
    "2. **GradCAM** remains a strong baseline for image attribution\n",
    "3. **Integrated Gradients** provides theoretically grounded text attributions\n",
    "4. **Multi-dataset evaluation** is crucial for robust conclusions\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "- DiET often identifies different important features compared to GradCAM/IG\n",
    "- The agreement between methods varies significantly across datasets\n",
    "- Perturbation-based metrics provide actionable evaluation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Try the framework on your own datasets\n",
    "2. Extend with additional XAI methods (SHAP, LIME, etc.)\n",
    "3. Add new evaluation metrics\n",
    "4. Contribute improvements to the repository!\n",
    "\n",
    "## Resources\n",
    "\n",
    "- üìö [DiET Paper (NeurIPS 2023)](https://arxiv.org/abs/2305.04249)\n",
    "- üìÅ [Repository](https://github.com/xMOROx/Machine-Learning-Project-2025-2026)\n",
    "- üìñ [Framework Documentation](../README.md)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Experimenting! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
