{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiET vs Basic XAI Methods: Comprehensive Comparison Framework\n",
    "\n",
    "## A Complete Experimental Pipeline for Image and Text Classification\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Machine Learning Research Team  \n",
    "**Date:** 2025-2026 Academic Year  \n",
    "**Course:** Advanced Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "### Abstract\n",
    "\n",
    "This notebook presents a comprehensive experimental comparison between **DiET (Distractor Erasure Tuning)** and standard XAI methods:\n",
    "\n",
    "- **Image Classification:** DiET vs GradCAM on CIFAR-10, CIFAR-100, SVHN, Fashion-MNIST\n",
    "- **Text Classification:** DiET vs Integrated Gradients on SST-2, IMDB, AG News\n",
    "\n",
    "The notebook provides robust evaluation metrics, visual summaries, and statistical analysis suitable for academic presentations.\n",
    "\n",
    "### Reference\n",
    "\n",
    "Bhalla, U., et al. (2023). **\"Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability.\"** *NeurIPS 2023.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Experimental Configuration](#2-experimental-configuration)\n",
    "3. [Image Experiments: DiET vs GradCAM](#3-image-experiments-diet-vs-gradcam)\n",
    "4. [Text Experiments: DiET vs Integrated Gradients](#4-text-experiments-diet-vs-integrated-gradients)\n",
    "5. [Combined Results Summary](#5-combined-results-summary)\n",
    "6. [Statistical Analysis](#6-statistical-analysis)\n",
    "7. [Export Results](#7-export-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Hardware Configuration\n",
    "\n",
    "This notebook is optimized for Google Colab Pro with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and type\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDWARE CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU Available: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    if gpu_memory >= 15:\n",
    "        print(\"\\nConfiguration: HIGH-MEMORY GPU\")\n",
    "        GPU_CONFIG = \"high\"\n",
    "    elif gpu_memory >= 8:\n",
    "        print(\"\\nConfiguration: STANDARD GPU\")\n",
    "        GPU_CONFIG = \"standard\"\n",
    "    else:\n",
    "        print(\"\\nConfiguration: LOW-MEMORY GPU\")\n",
    "        GPU_CONFIG = \"low\"\n",
    "else:\n",
    "    print(\"WARNING: No GPU available. Training will be slow.\")\n",
    "    print(\"Enable GPU: Runtime -> Change runtime type -> GPU\")\n",
    "    GPU_CONFIG = \"cpu\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nUsing device: {DEVICE.upper()}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/xMOROx/Machine-Learning-Project-2025-2026.git\"\n",
    "REPO_DIR = \"Machine-Learning-Project-2025-2026\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(\"Cloning repository...\")\n",
    "    !git clone --recursive {REPO_URL}\n",
    "    print(\"Repository cloned successfully.\")\n",
    "else:\n",
    "    print(\"Repository already exists. Pulling latest changes...\")\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "    !git submodule update --init --recursive\n",
    "    %cd ..\n",
    "\n",
    "%cd {REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets tqdm matplotlib seaborn pandas numpy pillow scikit-learn captum scipy\n",
    "\n",
    "print(\"All dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project to path and import modules\n",
    "import sys\n",
    "sys.path.insert(0, './scripts/xai_experiments')\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"All modules imported successfully.\")\n",
    "print(f\"Experiment started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Experimental Configuration\n",
    "\n",
    "### 2.1 Configuration Parameters\n",
    "\n",
    "Parameters are automatically adjusted based on available GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration based on GPU capabilities\n",
    "if GPU_CONFIG == \"high\":  # A100, V100\n",
    "    CONFIG = {\n",
    "        # Image settings\n",
    "        \"image_batch_size\": 128,\n",
    "        \"image_epochs\": 10,\n",
    "        \"image_max_samples\": 10000,\n",
    "        \"image_comparison_samples\": 200,\n",
    "        \"image_datasets\": [\"cifar10\", \"cifar100\", \"svhn\", \"fashion_mnist\"],\n",
    "        # Text settings\n",
    "        \"text_batch_size\": 32,\n",
    "        \"text_epochs\": 3,\n",
    "        \"text_max_length\": 256,\n",
    "        \"text_max_samples\": 3000,\n",
    "        \"text_comparison_samples\": 100,\n",
    "        \"text_datasets\": [\"sst2\", \"imdb\", \"ag_news\"],\n",
    "        \"text_top_k\": 10,\n",
    "    }\n",
    "elif GPU_CONFIG == \"standard\":  # T4, P100\n",
    "    CONFIG = {\n",
    "        \"image_batch_size\": 64,\n",
    "        \"image_epochs\": 5,\n",
    "        \"image_max_samples\": 5000,\n",
    "        \"image_comparison_samples\": 100,\n",
    "        \"image_datasets\": [\"cifar10\", \"cifar100\", \"svhn\", \"fashion_mnist\"],\n",
    "        \"text_batch_size\": 16,\n",
    "        \"text_epochs\": 2,\n",
    "        \"text_max_length\": 128,\n",
    "        \"text_max_samples\": 2000,\n",
    "        \"text_comparison_samples\": 50,\n",
    "        \"text_datasets\": [\"sst2\", \"imdb\", \"ag_news\"],\n",
    "        \"text_top_k\": 5,\n",
    "    }\n",
    "elif GPU_CONFIG == \"low\":  # K80, older GPUs\n",
    "    CONFIG = {\n",
    "        \"image_batch_size\": 32,\n",
    "        \"image_epochs\": 3,\n",
    "        \"image_max_samples\": 2000,\n",
    "        \"image_comparison_samples\": 50,\n",
    "        \"image_datasets\": [\"cifar10\", \"svhn\"],\n",
    "        \"text_batch_size\": 8,\n",
    "        \"text_epochs\": 2,\n",
    "        \"text_max_length\": 64,\n",
    "        \"text_max_samples\": 1000,\n",
    "        \"text_comparison_samples\": 30,\n",
    "        \"text_datasets\": [\"sst2\", \"ag_news\"],\n",
    "        \"text_top_k\": 5,\n",
    "    }\n",
    "else:  # CPU\n",
    "    CONFIG = {\n",
    "        \"image_batch_size\": 16,\n",
    "        \"image_epochs\": 2,\n",
    "        \"image_max_samples\": 1000,\n",
    "        \"image_comparison_samples\": 20,\n",
    "        \"image_datasets\": [\"cifar10\"],\n",
    "        \"text_batch_size\": 4,\n",
    "        \"text_epochs\": 1,\n",
    "        \"text_max_length\": 64,\n",
    "        \"text_max_samples\": 500,\n",
    "        \"text_comparison_samples\": 20,\n",
    "        \"text_datasets\": [\"sst2\"],\n",
    "        \"text_top_k\": 5,\n",
    "    }\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nImage Experiments:\")\n",
    "print(f\"  Datasets: {CONFIG['image_datasets']}\")\n",
    "print(f\"  Batch size: {CONFIG['image_batch_size']}\")\n",
    "print(f\"  Epochs: {CONFIG['image_epochs']}\")\n",
    "print(f\"  Max samples: {CONFIG['image_max_samples']}\")\n",
    "print(f\"  Comparison samples: {CONFIG['image_comparison_samples']}\")\n",
    "print(\"\\nText Experiments:\")\n",
    "print(f\"  Datasets: {CONFIG['text_datasets']}\")\n",
    "print(f\"  Batch size: {CONFIG['text_batch_size']}\")\n",
    "print(f\"  Epochs: {CONFIG['text_epochs']}\")\n",
    "print(f\"  Max length: {CONFIG['text_max_length']}\")\n",
    "print(f\"  Max samples: {CONFIG['text_max_samples']}\")\n",
    "print(f\"  Top-k tokens: {CONFIG['text_top_k']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize Comparison Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and configure the comparison framework\n",
    "from experiments.xai_comparison import XAIMethodsComparison, ComparisonConfig\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = \"./outputs/colab_experiments/comprehensive_comparison\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize configuration\n",
    "comparison_config = ComparisonConfig(\n",
    "    device=DEVICE,\n",
    "    # Image settings\n",
    "    image_datasets=CONFIG[\"image_datasets\"],\n",
    "    image_batch_size=CONFIG[\"image_batch_size\"],\n",
    "    image_epochs=CONFIG[\"image_epochs\"],\n",
    "    image_max_samples=CONFIG[\"image_max_samples\"],\n",
    "    image_comparison_samples=CONFIG[\"image_comparison_samples\"],\n",
    "    # Text settings\n",
    "    text_datasets=CONFIG[\"text_datasets\"],\n",
    "    text_batch_size=CONFIG[\"text_batch_size\"],\n",
    "    text_epochs=CONFIG[\"text_epochs\"],\n",
    "    text_max_length=CONFIG[\"text_max_length\"],\n",
    "    text_max_samples=CONFIG[\"text_max_samples\"],\n",
    "    text_comparison_samples=CONFIG[\"text_comparison_samples\"],\n",
    "    text_top_k=CONFIG[\"text_top_k\"],\n",
    "    low_vram=(GPU_CONFIG == \"low\"),\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "# Initialize comparison object\n",
    "comparison = XAIMethodsComparison(comparison_config)\n",
    "\n",
    "print(\"Comparison framework initialized.\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Image Experiments: DiET vs GradCAM\n",
    "\n",
    "### 3.1 Run Image Comparison Experiments\n",
    "\n",
    "This section compares DiET and GradCAM on image classification datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"IMAGE EXPERIMENTS: DiET vs GradCAM\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDatasets: {CONFIG['image_datasets']}\")\n",
    "print(f\"Samples per dataset: {CONFIG['image_max_samples']}\")\n",
    "print(f\"Training epochs: {CONFIG['image_epochs']}\")\n",
    "print(\"\\nStarting experiments...\\n\")\n",
    "\n",
    "image_start_time = datetime.now()\n",
    "\n",
    "# Run image experiments\n",
    "image_results = comparison.run_all_image_comparisons(skip_training=False)\n",
    "\n",
    "image_end_time = datetime.now()\n",
    "image_duration = (image_end_time - image_start_time).seconds\n",
    "\n",
    "print(f\"\\nImage experiments completed in {image_duration // 60} minutes {image_duration % 60} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Image Results: Visual Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image results\n",
    "image_data = []\n",
    "for dataset_name, result in image_results.items():\n",
    "    if \"error\" not in result:\n",
    "        image_data.append({\n",
    "            \"Dataset\": dataset_name.upper(),\n",
    "            \"Baseline Accuracy\": result.get(\"baseline_accuracy\", 0),\n",
    "            \"DiET Accuracy\": result.get(\"diet_accuracy\", 0),\n",
    "            \"GradCAM Score\": result.get(\"gradcam_mean_score\", 0),\n",
    "            \"DiET Score\": result.get(\"diet_mean_score\", 0),\n",
    "            \"Improvement\": result.get(\"improvement\", 0),\n",
    "            \"DiET Better\": result.get(\"diet_better\", False),\n",
    "        })\n",
    "\n",
    "image_df = pd.DataFrame(image_data)\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMAGE EXPERIMENTS: QUANTITATIVE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nPixel Perturbation Scores (higher = better attribution quality):\\n\")\n",
    "print(image_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "if len(image_df) > 0:\n",
    "    diet_wins = image_df[\"DiET Better\"].sum()\n",
    "    total = len(image_df)\n",
    "    avg_improvement = image_df[\"Improvement\"].mean()\n",
    "    print(f\"\\nSummary: DiET outperforms GradCAM on {diet_wins}/{total} datasets\")\n",
    "    print(f\"Average improvement: {avg_improvement:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Summary: Image Experiments\n",
    "if len(image_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"Image Experiments: DiET vs GradCAM - Visual Summary\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Attribution Quality Comparison\n",
    "    datasets = image_df[\"Dataset\"].tolist()\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0, 0].bar(x - width/2, image_df[\"GradCAM Score\"], width, label='GradCAM', color='#2196F3')\n",
    "    bars2 = axes[0, 0].bar(x + width/2, image_df[\"DiET Score\"], width, label='DiET', color='#4CAF50')\n",
    "    axes[0, 0].set_ylabel('Pixel Perturbation Score')\n",
    "    axes[0, 0].set_title('Attribution Quality Comparison (Higher = Better)')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(datasets)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    for bar in bars1:\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{bar.get_height():.2f}', ha='center', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{bar.get_height():.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Improvement Over GradCAM\n",
    "    improvements = image_df[\"Improvement\"].tolist()\n",
    "    colors = ['#4CAF50' if imp > 0 else '#F44336' for imp in improvements]\n",
    "    axes[0, 1].barh(datasets, improvements, color=colors)\n",
    "    axes[0, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[0, 1].set_xlabel('Improvement (DiET - GradCAM)')\n",
    "    axes[0, 1].set_title('DiET Improvement Over GradCAM')\n",
    "    for i, v in enumerate(improvements):\n",
    "        axes[0, 1].text(v + 0.005 if v >= 0 else v - 0.005, i, f'{v:+.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    # Plot 3: Model Accuracy Comparison\n",
    "    x = np.arange(len(datasets))\n",
    "    bars3 = axes[1, 0].bar(x - width/2, image_df[\"Baseline Accuracy\"], width, label='Baseline', color='#FF9800')\n",
    "    bars4 = axes[1, 0].bar(x + width/2, image_df[\"DiET Accuracy\"], width, label='After DiET', color='#9C27B0')\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 0].set_title('Model Accuracy Before and After DiET')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(datasets)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_ylim(0, 100)\n",
    "    \n",
    "    # Plot 4: Summary Pie Chart\n",
    "    diet_wins = image_df[\"DiET Better\"].sum()\n",
    "    gradcam_wins = len(image_df) - diet_wins\n",
    "    axes[1, 1].pie([diet_wins, gradcam_wins], labels=['DiET Better', 'GradCAM Better'], \n",
    "                   autopct='%1.0f%%', colors=['#4CAF50', '#2196F3'], startangle=90)\n",
    "    axes[1, 1].set_title('Method Performance Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/image_visual_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFigure saved: {OUTPUT_DIR}/image_visual_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Text Experiments: DiET vs Integrated Gradients\n",
    "\n",
    "### 4.1 Run Text Comparison Experiments\n",
    "\n",
    "This section compares DiET and Integrated Gradients on text classification datasets using BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEXT EXPERIMENTS: DiET vs Integrated Gradients\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDatasets: {CONFIG['text_datasets']}\")\n",
    "print(f\"Model: BERT-base-uncased\")\n",
    "print(f\"Max sequence length: {CONFIG['text_max_length']}\")\n",
    "print(f\"Training epochs: {CONFIG['text_epochs']}\")\n",
    "print(\"\\nStarting experiments...\\n\")\n",
    "\n",
    "text_start_time = datetime.now()\n",
    "\n",
    "# Run text experiments\n",
    "text_results = comparison.run_all_text_comparisons(skip_training=False)\n",
    "\n",
    "text_end_time = datetime.now()\n",
    "text_duration = (text_end_time - text_start_time).seconds\n",
    "\n",
    "print(f\"\\nText experiments completed in {text_duration // 60} minutes {text_duration % 60} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text Results: Visual Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text results\n",
    "text_data = []\n",
    "for dataset_name, result in text_results.items():\n",
    "    if \"error\" not in result:\n",
    "        text_data.append({\n",
    "            \"Dataset\": dataset_name.upper(),\n",
    "            \"Baseline Accuracy\": result.get(\"baseline_accuracy\", 0),\n",
    "            \"IG-DiET Overlap\": result.get(\"ig_diet_overlap\", 0),\n",
    "            \"Samples Compared\": result.get(\"samples_compared\", 0),\n",
    "        })\n",
    "\n",
    "text_df = pd.DataFrame(text_data)\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEXT EXPERIMENTS: QUANTITATIVE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTop-{CONFIG['text_top_k']} Token Overlap between IG and DiET:\\n\")\n",
    "print(text_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "if len(text_df) > 0:\n",
    "    avg_overlap = text_df[\"IG-DiET Overlap\"].mean()\n",
    "    avg_accuracy = text_df[\"Baseline Accuracy\"].mean()\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Average BERT accuracy: {avg_accuracy:.1f}%\")\n",
    "    print(f\"  Average IG-DiET overlap: {avg_overlap:.4f}\")\n",
    "    \n",
    "    if avg_overlap >= 0.5:\n",
    "        print(\"  Interpretation: Methods show good agreement on important tokens\")\n",
    "    else:\n",
    "        print(\"  Interpretation: Methods identify different features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Summary: Text Experiments\n",
    "if len(text_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(\"Text Experiments: DiET vs Integrated Gradients - Visual Summary\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    datasets = text_df[\"Dataset\"].tolist()\n",
    "    \n",
    "    # Plot 1: IG-DiET Token Overlap\n",
    "    overlaps = text_df[\"IG-DiET Overlap\"].tolist()\n",
    "    colors = ['#4CAF50' if o >= 0.5 else '#FF9800' if o >= 0.3 else '#F44336' for o in overlaps]\n",
    "    bars = axes[0].bar(datasets, overlaps, color=colors, edgecolor='black')\n",
    "    axes[0].axhline(y=0.5, color='gray', linestyle='--', linewidth=1, label='50% threshold')\n",
    "    axes[0].set_ylabel(f'Top-{CONFIG[\"text_top_k\"]} Token Overlap')\n",
    "    axes[0].set_title('IG-DiET Attribution Agreement')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend()\n",
    "    for bar, val in zip(bars, overlaps):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{val:.3f}', ha='center', fontsize=11)\n",
    "    \n",
    "    # Plot 2: BERT Accuracy\n",
    "    accuracies = text_df[\"Baseline Accuracy\"].tolist()\n",
    "    bars2 = axes[1].bar(datasets, accuracies, color='#2196F3', edgecolor='black')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('BERT Classification Accuracy')\n",
    "    axes[1].set_ylim(0, 100)\n",
    "    for bar, acc in zip(bars2, accuracies):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{acc:.1f}%', ha='center', fontsize=11)\n",
    "    \n",
    "    # Plot 3: Agreement Level Distribution\n",
    "    high_agreement = sum(1 for o in overlaps if o >= 0.5)\n",
    "    medium_agreement = sum(1 for o in overlaps if 0.3 <= o < 0.5)\n",
    "    low_agreement = sum(1 for o in overlaps if o < 0.3)\n",
    "    \n",
    "    labels = ['High (>=0.5)', 'Medium (0.3-0.5)', 'Low (<0.3)']\n",
    "    sizes = [high_agreement, medium_agreement, low_agreement]\n",
    "    colors_pie = ['#4CAF50', '#FF9800', '#F44336']\n",
    "    \n",
    "    # Only show non-zero segments\n",
    "    non_zero = [(l, s, c) for l, s, c in zip(labels, sizes, colors_pie) if s > 0]\n",
    "    if non_zero:\n",
    "        labels, sizes, colors_pie = zip(*non_zero)\n",
    "        axes[2].pie(sizes, labels=labels, autopct='%1.0f%%', colors=colors_pie, startangle=90)\n",
    "    axes[2].set_title('Agreement Level Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/text_visual_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFigure saved: {OUTPUT_DIR}/text_visual_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Combined Results Summary\n",
    "\n",
    "### 5.1 Complete Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate combined summary report\n",
    "print(\"=\" * 70)\n",
    "print(\"COMBINED EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get full results dataframe\n",
    "full_df = comparison.get_results_dataframe()\n",
    "print(\"\\nComplete Results Table:\\n\")\n",
    "print(full_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report text\n",
    "report = comparison.generate_summary_report()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Combined Visual Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Visual Summary\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "fig.suptitle(\"DiET vs Basic XAI Methods: Complete Comparison Summary\", fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# Create grid\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "# Image comparison subplot\n",
    "if len(image_df) > 0:\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    datasets = image_df[\"Dataset\"].tolist()\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    ax1.bar(x - width/2, image_df[\"GradCAM Score\"], width, label='GradCAM', color='#2196F3')\n",
    "    ax1.bar(x + width/2, image_df[\"DiET Score\"], width, label='DiET', color='#4CAF50')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Image: DiET vs GradCAM')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1)\n",
    "\n",
    "# Text comparison subplot\n",
    "if len(text_df) > 0:\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    datasets = text_df[\"Dataset\"].tolist()\n",
    "    overlaps = text_df[\"IG-DiET Overlap\"].tolist()\n",
    "    colors = ['#4CAF50' if o >= 0.5 else '#FF9800' for o in overlaps]\n",
    "    ax2.bar(datasets, overlaps, color=colors, edgecolor='black')\n",
    "    ax2.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    ax2.set_ylabel('Token Overlap')\n",
    "    ax2.set_title('Text: IG-DiET Agreement')\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "# Image accuracy subplot\n",
    "if len(image_df) > 0:\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    datasets = image_df[\"Dataset\"].tolist()\n",
    "    x = np.arange(len(datasets))\n",
    "    ax3.bar(x - width/2, image_df[\"Baseline Accuracy\"], width, label='Baseline', color='#FF9800')\n",
    "    ax3.bar(x + width/2, image_df[\"DiET Accuracy\"], width, label='DiET', color='#9C27B0')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.set_title('Image Model Accuracy')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    ax3.legend()\n",
    "\n",
    "# Text accuracy subplot\n",
    "if len(text_df) > 0:\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    datasets = text_df[\"Dataset\"].tolist()\n",
    "    accuracies = text_df[\"Baseline Accuracy\"].tolist()\n",
    "    ax4.bar(datasets, accuracies, color='#2196F3', edgecolor='black')\n",
    "    ax4.set_ylabel('Accuracy (%)')\n",
    "    ax4.set_title('Text Model Accuracy (BERT)')\n",
    "    ax4.set_ylim(0, 100)\n",
    "\n",
    "# Summary statistics subplot\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "ax5.axis('off')\n",
    "\n",
    "# Create summary text\n",
    "summary_text = \"EXPERIMENT SUMMARY\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "\n",
    "if len(image_df) > 0:\n",
    "    diet_wins = image_df[\"DiET Better\"].sum()\n",
    "    avg_imp = image_df[\"Improvement\"].mean()\n",
    "    summary_text += f\"IMAGE EXPERIMENTS (DiET vs GradCAM):\\n\"\n",
    "    summary_text += f\"  - DiET outperforms GradCAM: {diet_wins}/{len(image_df)} datasets\\n\"\n",
    "    summary_text += f\"  - Average improvement: {avg_imp:+.4f}\\n\\n\"\n",
    "\n",
    "if len(text_df) > 0:\n",
    "    avg_overlap = text_df[\"IG-DiET Overlap\"].mean()\n",
    "    avg_acc = text_df[\"Baseline Accuracy\"].mean()\n",
    "    summary_text += f\"TEXT EXPERIMENTS (DiET vs Integrated Gradients):\\n\"\n",
    "    summary_text += f\"  - Average IG-DiET overlap: {avg_overlap:.4f}\\n\"\n",
    "    summary_text += f\"  - Average BERT accuracy: {avg_acc:.1f}%\\n\"\n",
    "\n",
    "ax5.text(0.5, 0.5, summary_text, transform=ax5.transAxes, fontsize=12,\n",
    "         verticalalignment='center', horizontalalignment='center',\n",
    "         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig(f'{OUTPUT_DIR}/combined_visual_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {OUTPUT_DIR}/combined_visual_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Statistical Analysis\n",
    "\n",
    "### 6.1 Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Image experiments statistical analysis\n",
    "if len(image_df) >= 3:\n",
    "    gradcam_scores = image_df[\"GradCAM Score\"].values\n",
    "    diet_scores = image_df[\"DiET Score\"].values\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(diet_scores, gradcam_scores)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((np.var(gradcam_scores) + np.var(diet_scores)) / 2)\n",
    "    cohens_d = (np.mean(diet_scores) - np.mean(gradcam_scores)) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    print(\"\\nImage Experiments (DiET vs GradCAM):\")\n",
    "    print(f\"  Paired t-test:\")\n",
    "    print(f\"    t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"    p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"    Result: Statistically significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"    Result: Not statistically significant\")\n",
    "    \n",
    "    print(f\"\\n  Effect Size (Cohen's d): {cohens_d:.4f}\")\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        print(\"    Interpretation: Small effect\")\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        print(\"    Interpretation: Medium effect\")\n",
    "    else:\n",
    "        print(\"    Interpretation: Large effect\")\n",
    "else:\n",
    "    print(\"\\nImage Experiments: Not enough data points for statistical testing (need >= 3)\")\n",
    "\n",
    "# Text experiments statistical analysis\n",
    "if len(text_df) >= 2:\n",
    "    overlaps = text_df[\"IG-DiET Overlap\"].values\n",
    "    \n",
    "    # One-sample t-test against 0.5 threshold\n",
    "    t_stat_text, p_value_text = stats.ttest_1samp(overlaps, 0.5)\n",
    "    \n",
    "    print(\"\\nText Experiments (IG-DiET Overlap):\")\n",
    "    print(f\"  One-sample t-test (vs 0.5 threshold):\")\n",
    "    print(f\"    Mean overlap: {np.mean(overlaps):.4f}\")\n",
    "    print(f\"    t-statistic: {t_stat_text:.4f}\")\n",
    "    print(f\"    p-value: {p_value_text:.4f}\")\n",
    "    \n",
    "    if np.mean(overlaps) >= 0.5:\n",
    "        print(\"    Interpretation: Methods show agreement above chance level\")\n",
    "    else:\n",
    "        print(\"    Interpretation: Methods identify different features\")\n",
    "else:\n",
    "    print(\"\\nText Experiments: Not enough data points for statistical testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Export Results\n",
    "\n",
    "### 7.1 Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "comparison.save_results()\n",
    "\n",
    "# Save additional CSV files\n",
    "if len(image_df) > 0:\n",
    "    image_df.to_csv(f'{OUTPUT_DIR}/image_results.csv', index=False)\n",
    "if len(text_df) > 0:\n",
    "    text_df.to_csv(f'{OUTPUT_DIR}/text_results.csv', index=False)\n",
    "if len(full_df) > 0:\n",
    "    full_df.to_csv(f'{OUTPUT_DIR}/all_results.csv', index=False)\n",
    "\n",
    "# Save configuration\n",
    "with open(f'{OUTPUT_DIR}/experiment_config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved:\")\n",
    "print(f\"  - {OUTPUT_DIR}/comparison_results.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/image_results.csv\")\n",
    "print(f\"  - {OUTPUT_DIR}/text_results.csv\")\n",
    "print(f\"  - {OUTPUT_DIR}/all_results.csv\")\n",
    "print(f\"  - {OUTPUT_DIR}/experiment_config.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/image_visual_summary.png\")\n",
    "print(f\"  - {OUTPUT_DIR}/text_visual_summary.png\")\n",
    "print(f\"  - {OUTPUT_DIR}/combined_visual_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations from framework\n",
    "try:\n",
    "    viz_files = comparison.visualize_results(save_plots=True, show=False)\n",
    "    print(\"\\nAdditional visualizations generated:\")\n",
    "    for name, path in viz_files.items():\n",
    "        print(f\"  - {name}: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Some visualizations could not be generated: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Create zip of all results\n",
    "    !zip -r comprehensive_results.zip {OUTPUT_DIR}/\n",
    "    \n",
    "    print(\"\\nDownload your results:\")\n",
    "    files.download('comprehensive_results.zip')\n",
    "except:\n",
    "    print(f\"\\nResults are saved locally in: {OUTPUT_DIR}/\")\n",
    "    print(\"(Download option only available in Google Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "total_duration = (image_duration if 'image_duration' in dir() else 0) + (text_duration if 'text_duration' in dir() else 0)\n",
    "\n",
    "final_report = f\"\"\"\n",
    "================================================================================\n",
    "                    DiET vs BASIC XAI METHODS\n",
    "                    COMPREHENSIVE COMPARISON REPORT\n",
    "================================================================================\n",
    "\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Device: {DEVICE} ({GPU_CONFIG})\n",
    "Total Duration: {total_duration // 60} minutes {total_duration % 60} seconds\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "IMAGE EXPERIMENTS: DiET vs GradCAM\n",
    "--------------------------------------------------------------------------------\n",
    "Datasets: {CONFIG['image_datasets']}\n",
    "Model: ResNet\n",
    "Training epochs: {CONFIG['image_epochs']}\n",
    "Comparison samples: {CONFIG['image_comparison_samples']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if len(image_df) > 0:\n",
    "    for _, row in image_df.iterrows():\n",
    "        status = \"[+]\" if row[\"DiET Better\"] else \"[-]\"\n",
    "        final_report += f\"{status} {row['Dataset']}: GradCAM={row['GradCAM Score']:.4f}, DiET={row['DiET Score']:.4f}, Improvement={row['Improvement']:+.4f}\\n\"\n",
    "    \n",
    "    diet_wins = image_df[\"DiET Better\"].sum()\n",
    "    final_report += f\"\\nSummary: DiET outperforms GradCAM on {diet_wins}/{len(image_df)} datasets\\n\"\n",
    "\n",
    "final_report += f\"\"\"\n",
    "--------------------------------------------------------------------------------\n",
    "TEXT EXPERIMENTS: DiET vs Integrated Gradients\n",
    "--------------------------------------------------------------------------------\n",
    "Datasets: {CONFIG['text_datasets']}\n",
    "Model: BERT-base-uncased\n",
    "Max length: {CONFIG['text_max_length']}\n",
    "Training epochs: {CONFIG['text_epochs']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if len(text_df) > 0:\n",
    "    for _, row in text_df.iterrows():\n",
    "        level = \"[HIGH]\" if row[\"IG-DiET Overlap\"] >= 0.5 else \"[MED]\" if row[\"IG-DiET Overlap\"] >= 0.3 else \"[LOW]\"\n",
    "        final_report += f\"{level} {row['Dataset']}: Accuracy={row['Baseline Accuracy']:.1f}%, Overlap={row['IG-DiET Overlap']:.4f}\\n\"\n",
    "    \n",
    "    avg_overlap = text_df[\"IG-DiET Overlap\"].mean()\n",
    "    final_report += f\"\\nSummary: Average IG-DiET overlap = {avg_overlap:.4f}\\n\"\n",
    "\n",
    "final_report += f\"\"\"\n",
    "================================================================================\n",
    "Output Files:\n",
    "  - {OUTPUT_DIR}/comparison_results.json\n",
    "  - {OUTPUT_DIR}/image_results.csv\n",
    "  - {OUTPUT_DIR}/text_results.csv\n",
    "  - {OUTPUT_DIR}/all_results.csv\n",
    "  - {OUTPUT_DIR}/image_visual_summary.png\n",
    "  - {OUTPUT_DIR}/text_visual_summary.png\n",
    "  - {OUTPUT_DIR}/combined_visual_summary.png\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Save final report\n",
    "with open(f'{OUTPUT_DIR}/final_report.txt', 'w') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Bhalla, U., et al. (2023). \"Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability.\" *NeurIPS 2023.*\n",
    "\n",
    "2. Selvaraju, R. R., et al. (2017). \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.\" *ICCV 2017.*\n",
    "\n",
    "3. Sundararajan, M., Taly, A., & Yan, Q. (2017). \"Axiomatic Attribution for Deep Networks.\" *ICML 2017.*\n",
    "\n",
    "4. Devlin, J., et al. (2019). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" *NAACL 2019.*\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 1.0  \n",
    "**Last Updated:** 2025-2026 Academic Year  \n",
    "**Repository:** https://github.com/xMOROx/Machine-Learning-Project-2025-2026"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
